import time
import streamlit as st
import base64
from streamlit.components.v1 import html
import os
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import re
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¸­æ–‡æ£€æµ‹å‡½æ•°
def contains_chinese(text):
    for char in text:
        if '\u4e00' <= char <= '\u9fff':
            return True
    return False

# é…ç½®è®¾ç½®
MODEL_ROOT = "../FuncCodes/predict_code/500 words/models/"  # æ¨¡å‹æ ¹ç›®å½•
DIMENSIONS = ['IE', 'NS', 'TF', 'JP']  # å››ä¸ªç»´åº¦
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# æ·»åŠ åŒè¯­æ¨¡å‹ç±»
class BilingualMBTITester:
    def __init__(self):
        self.models = {}
        self.tokenizer = None
        self._load_models()

    def _load_models(self):
        model_paths = {
            'IE': '../FuncCodes/predict_code/chinese & english/models/IE_model',
            'NS': '../FuncCodes/predict_code/chinese & english/models/NS_model',
            'TF': '../FuncCodes/predict_code/chinese & english/models/TF_model',
            'JP': '../FuncCodes/predict_code/chinese & english/models/JP_model'
        }
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained("../FuncCodes/predict_code/chinese & english/xlm-roberta-base/")
        
        # åŠ è½½æ¨¡å‹
        for dimension, path in model_paths.items():
            model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=2)
            self.models[dimension] = model.to(DEVICE)
            print(f"æˆåŠŸåŠ è½½åŒè¯­æ¨¡å‹ {dimension} ç»´åº¦")
        print("æ‰€æœ‰åŒè¯­æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def predict(self, text):
        processed_text = preprocess_text(text)
        
        encoding = self.tokenizer(
            processed_text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(DEVICE)
        attention_mask = encoding['attention_mask'].to(DEVICE)

        predictions = {}
        confidences = {}
        
        for dim, model in self.models.items():
            model.eval()
            with torch.no_grad():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)
                pred = torch.argmax(logits, dim=1).item()
                confidence = int(probs[0][pred].item() * 100)
                
                predictions[dim] = pred
                confidences[dim] = confidence

        # è·å–è¯è¯­è´¡çŒ®åº¦
        top_contributing_words = self._get_embedding_contributions(input_ids, attention_mask)
        
        return self._format_mbti(predictions), confidences, top_contributing_words

    def _get_embedding_contributions(self, input_ids, attention_mask):
        word_contributions = {}
        words = self.tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())
        
        embedding_layer = self.models['IE'].get_input_embeddings()
        embeddings = embedding_layer(input_ids)
        
        for i, word in enumerate(words):
            if (word not in self.tokenizer.all_special_tokens and 
                "æˆ‘" not in word and 
                not (len(word) <= 3 and word.startswith("çš„")) and 
                len(word) >= 2):
                word_embedding = embeddings[0, i].cpu().detach().numpy()
                importance = np.linalg.norm(word_embedding)
                word_contributions[word] = importance
        
        return sorted(word_contributions.items(), key=lambda x: x[1], reverse=True)[:5]

    def _format_mbti(self, predictions):
        type_map = {
            'IE': {0: 'I', 1: 'E'},
            'NS': {0: 'N', 1: 'S'},
            'TF': {0: 'T', 1: 'F'},
            'JP': {0: 'J', 1: 'P'}
        }
        return ''.join([type_map[dim][pred] for dim, pred in predictions.items()])
    
# MBTITester ç±»å®šä¹‰
class MBTITester:
    def __init__(self):
        self.models = {}
        self.tokenizer = None
        self._load_models()

    def _load_single_model(self, dimension):
        model_path = os.path.join(MODEL_ROOT, f"{dimension}_model")
        
        if not self.tokenizer:
            self.tokenizer = BertTokenizer.from_pretrained(
                model_path,
                config=os.path.join(model_path, "config.json"),
                tokenizer_config_file=os.path.join(model_path, "tokenizer_config.json"),
                vocab_file=os.path.join(model_path, "vocab.txt")
            )
        
        model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True)
        return model.to(DEVICE)

    def _load_models(self):
        for dim in DIMENSIONS:
            self.models[dim] = self._load_single_model(dim)
            print(f"æˆåŠŸåŠ è½½ {dim} ç»´åº¦æ¨¡å‹")
        print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def predict(self, text):
        processed_text = preprocess_text(text)

        encoding = self.tokenizer(
            processed_text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        ).to(DEVICE)

        predictions = {}
        confidences = {}
        word_contributions = {}

        with torch.no_grad():
            for dim, model in self.models.items():
                outputs = model(**encoding)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)
                pred = torch.argmax(probs).item()
                confidence = int(probs[0][pred].item() * 100)

                predictions[dim] = pred
                confidences[dim] = confidence

                attention_weights = outputs.attentions
                word_contributions[dim] = self._compute_attention_contributions(attention_weights, encoding)

        total_word_contributions = self._aggregate_word_contributions(word_contributions)
        top_contributing_words = self._get_top_contributing_words(total_word_contributions)

        return self._format_mbti(predictions), confidences, top_contributing_words

    def _compute_attention_contributions(self, attention_weights, encoding):
        tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'][0].cpu().numpy())
        tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]
        word_contributions = {token: 0 for token in tokens}

        for layer in range(len(attention_weights)):
            for head in range(attention_weights[layer].shape[1]):
                attention_map = attention_weights[layer][0, head].detach().cpu().numpy()
                for i, token in enumerate(tokens):
                    word_contributions[token] += np.sum(attention_map[i, :])
        
        return word_contributions

    def _aggregate_word_contributions(self, word_contributions):
        total_word_contributions = {}
        for dim, contributions in word_contributions.items():
            for word, contribution in contributions.items():
                if word not in total_word_contributions:
                    total_word_contributions[word] = 0
                total_word_contributions[word] += contribution
        return total_word_contributions

    def _get_top_contributing_words(self, total_word_contributions, top_n=5):
        top_contributing_words = sorted(total_word_contributions.items(), key=lambda item: item[1], reverse=True)[:top_n]
        return top_contributing_words

    def _format_mbti(self, predictions):
        type_map = {
            'IE': {0: 'I', 1: 'E'},
            'NS': {0: 'N', 1: 'S'},
            'TF': {0: 'T', 1: 'F'},
            'JP': {0: 'J', 1: 'P'}
        }
        return ''.join([type_map[dim][pred] for dim, pred in predictions.items()])

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
if 'tester' not in st.session_state:
    st.session_state.tester = MBTITester()
if 'bilingual_tester' not in st.session_state:
    st.session_state.bilingual_tester = BilingualMBTITester()

def get_image_base64(image_path):
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode()
    return encoded_string

st.markdown("""
    <style>
    [data-testid="stImage"] {
        margin-top: -60px !important;
    }
    </style>
    """, unsafe_allow_html=True)

st.image("images/beamer.svg",use_container_width=True)

# æ·»åŠ å“åº”å¼æ–‡æœ¬æ ·å¼
st.markdown(
    """
    <style>
    button[kind="header"] {
        display: none;
    }
    * {
        font-family: "PingFang SC", "Source Sans Pro", -apple-system, BlinkMacSystemFont, sans-serif !important;
    }
    .mbtitest-page-bigtitle {
        font-size: calc(36px + 0.25vw) !important;
        margin-bottom: calc(0px + 0vw) !important;
        font-weight: bold !important;
        margin-top: calc(-20px + 0.25vw) !important;
    }
    .mbtitest-page-title {
        font-size: calc(16px + 0.25vw) !important;
        margin-bottom: calc(8px + 0.25vw) !important;
        font-weight: bold !important;
    }
    .mbtitest-page-type {
        margin-top: calc(8px + 0.25vw) !important;
        font-size: calc(14px + 0.25vw) !important;
        margin-bottom: calc(8px + 0.25vw) !important;
        font-weight: bold !important;
    }
    .mbtitest-page-text {
        font-size: calc(12px + 0.25vw) !important;
        margin-bottom: calc(12px + 0.25vw) !important;
    }
    
    .mbtitest-card {
        background: white;
        border-radius: 20px;
        padding: 20px 20px 10px 20px;
        margin: 10px 10px 10px 10px;
        transition: all 0.3s ease;
        border: 1.5px solid #eee;
    }
    
    .mbtitest-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    .mbtitest-page-typetitle {
        font-size: calc(14px + 0.25vw) !important;
        font-weight: bold !important;
        color: #333;
        margin-bottom: 1px !important;
        text-align: center;
    }
    .mbtitest-page-typetitle2 {
        font-size: calc(13px + 0.25vw) !important;
        font-weight: bold !important;
        color: #333;
        margin-bottom: 15px !important;
        text-align: center;
    }
    .mbtitest-card-image {
        text-align: center;
        margin: 10px 0;
    }
    
    .mbtitest-card-text {
        font-size: calc(11px + 0.25vw) !important;
        color: #666;
        line-height: 1.6;
        margin: 10px 10px 10px 10px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown('<p class="mbtitest-page-bigtitle">ğŸ¯ MBTI æ€§æ ¼é¢„æµ‹</p>', unsafe_allow_html=True)
st.caption("ğŸš€ é€šè¿‡ä¸€æ®µæ–‡æœ¬ï¼ŒMBTInsightå¯ä»¥è‡ªåŠ¨é¢„æµ‹å‡ºä½ çš„MBTIæ€§æ ¼ç±»å‹ã€‚")

# æ–‡æœ¬è¾“å…¥åŒºåŸŸ
user_input = st.text_area(
    "åœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³è¦åˆ†æçš„æ–‡æœ¬:",
    height=150,
    placeholder="è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„è‹±æ–‡æ–‡æœ¬..."
)

# æ ¹æ®è¾“å…¥è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ¨¡å‹
default_model_index = 1 if contains_chinese(user_input) else 0

# æ¨¡å‹é€‰æ‹©
model_type = st.selectbox(
    "é€‰æ‹©é¢„æµ‹æ¨¡å‹:",
    ["è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§", "åŒè¯­ç‰ˆæ¨¡å‹ ğŸ‡¨ğŸ‡³ ğŸ‡¬ğŸ‡§"],
    index=default_model_index
)

# æ ¹æ®å½“å‰é€‰æ‹©çš„æ¨¡å‹ç±»å‹æ›´æ–°å ä½ç¬¦æ–‡æœ¬
placeholder_text = "è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„" + ("è‹±æ–‡æ–‡æœ¬..." if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§" else "ä¸­è‹±æ–‡æ–‡æœ¬...")


# å¼€å§‹æŒ‰é’®
if st.button("å¼€å§‹åˆ†æ", type="primary"):
    if len(user_input.strip()) < 100:
        st.error("âš ï¸ è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„æ–‡æœ¬ä»¥ç¡®ä¿é¢„æµ‹å‡†ç¡®æ€§")
    else:
        with st.spinner("æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
            progress_bar = st.progress(0)
            
            try:
                # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç±»å‹è¿›è¡Œé¢„æµ‹
                if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§":
                    prediction, confidences, top_contributing_words = st.session_state.tester.predict(user_input)
                else:
                    prediction, confidences, top_contributing_words = st.session_state.bilingual_tester.predict(user_input)
                
                # æ›´æ–°è¿›åº¦æ¡
                for i in range(100):
                    time.sleep(0.02)
                    progress_bar.progress(i + 1)
                
                success = True
            except Exception as e:
                st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                success = False
            
            progress_bar.empty()
            
            if success:
                st.success("âœ¨ åˆ†æå®Œæˆï¼")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**é€‰ç”¨æ¨¡å‹ï¼š**")
                    st.markdown(model_type)
                with col2:
                    st.markdown("**é¢„æµ‹ç±»å‹ï¼š**")
                    st.markdown(f"**{prediction}**")

                # æ˜¾ç¤ºå„ç»´åº¦ç½®ä¿¡åº¦
                st.markdown("### ç»´åº¦åˆ†æ")
                cols = st.columns(4)
                dimensions = ['I/E', 'N/S', 'T/F', 'J/P']
                for i, (dim, col) in enumerate(zip(dimensions, cols)):
                    with col:
                        dim_key = ['IE', 'NS', 'TF', 'JP'][i]
                        st.metric(
                            label=dim,
                            value=f"{confidences[dim_key]}%"
                        )

                # æ˜¾ç¤ºå…³é”®è¯åˆ†æ
                st.markdown("### å…³é”®è¯åˆ†æ")
                st.markdown("å¯¹é¢„æµ‹ç»“æœå½±å“æœ€å¤§çš„è¯è¯­ï¼š")
                for word, _ in top_contributing_words:
                    st.markdown(f"- {word}")

else:
    placeholder_text = "è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„" + ("è‹±æ–‡æ–‡æœ¬..." if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§ " else "ä¸­è‹±æ–‡æ–‡æœ¬...")
    st.info(f"ğŸ‘† è¯·åœ¨ä¸Šæ–¹è¾“å…¥{placeholder_text}")