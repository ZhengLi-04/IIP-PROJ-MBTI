import time
import streamlit as st
import base64
from streamlit.components.v1 import html
import os
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import re
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import matplotlib.pyplot as plt
from wordcloud import WordCloud

st.logo("full_logo.png", size="large")


def is_chinese(word):
    return all('\u4e00' <= char <= '\u9fff' for char in word)

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¸­æ–‡æ£€æµ‹å‡½æ•°
def contains_chinese(text):
    for char in text:
        if '\u4e00' <= char <= '\u9fff':
            return True
    return False

def filter_words(words):
    return [item for item in words if 'æˆ‘' not in item[0]]  
# é…ç½®è®¾ç½®
MODEL_ROOT = "../FuncCodes/predict_code/500 words/models/"  # æ¨¡å‹æ ¹ç›®å½•
DIMENSIONS = ['IE', 'NS', 'TF', 'JP']  # å››ä¸ªç»´åº¦
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+', '', text)

    # æ–°å¢çš„é¢„å¤„ç†æ­¥éª¤
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)  # åªä¿ç•™æ–‡å­—ã€æ•°å­—å’Œä¸­æ–‡
    text = re.sub(r'\d+', ' ', text)  # ç§»é™¤æ•°å­—
    text = text.lower()  # è½¬æ¢ä¸ºå°å†™
    
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# æ·»åŠ åŒè¯­æ¨¡å‹ç±»
class BilingualMBTITester:
    def __init__(self):
        self.models = {}
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._load_models()
        self.stopwords = {"æˆ‘", "æˆ‘åœ¨", "æˆ‘è¿˜", "æˆ‘æ˜¯", "æˆ‘ä»¬", "çš„", "äº†", "æ˜¯", "åœ¨", "æœ‰"}

    def _load_models(self):
        model_paths = {
            'IE': '../FuncCodes/predict_code/chinese & english/models/IE_model',
            'NS': '../FuncCodes/predict_code/chinese & english/models/NS_model',
            'TF': '../FuncCodes/predict_code/chinese & english/models/TF_model',
            'JP': '../FuncCodes/predict_code/chinese & english/models/JP_model'
        }
        
        self.tokenizer = AutoTokenizer.from_pretrained("../FuncCodes/predict_code/chinese & english/xlm-roberta-base/")
        self.models = self._load_all_models(model_paths)

    def _load_all_models(self, model_paths):
        models = {}
        for dim, path in model_paths.items():
            model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=2)
            model.to(self.device)
            models[dim] = model
            print(f"æˆåŠŸåŠ è½½åŒè¯­æ¨¡å‹ {dim} ç»´åº¦")
        return models

    def predict(self, text):
        processed_text = self._preprocess_text(text)
        encoding = self.tokenizer(
            processed_text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)

        dimension_results = {}
        dimension_probs = {}
        
        for dim, model in self.models.items():
            model.eval()
            with torch.no_grad():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)
                pred = torch.argmax(logits, dim=1).item()
                dimension_results[dim] = pred
                dimension_probs[dim] = int(probs[0][pred].item() * 100)

        mbti_type = self._format_mbti(dimension_results)
        # formatted_output = self._format_output(mbti_type, dimension_results, dimension_probs)
        top_contributing_words = self.get_embedding_contributions(input_ids, attention_mask)
        
        return mbti_type, dimension_probs, top_contributing_words

    def _preprocess_text(self, text):
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'@\w+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _format_mbti(self, preds):
        return ("I" if preds['IE'] == 0 else "E") + \
               ("N" if preds['NS'] == 0 else "S") + \
               ("T" if preds['TF'] == 0 else "F") + \
               ("J" if preds['JP'] == 0 else "P")

    # def _format_output(self, mbti_type, results, probs):
    #     output = f"é¢„æµ‹çš„MBTIç±»å‹: {mbti_type}ï¼›"
    #     output += f"{'I' if results['IE']==0 else 'E'}ï¼ˆç½®ä¿¡åº¦ {probs['IE']}%ï¼‰"
    #     output += f"{'N' if results['NS']==0 else 'S'}ï¼ˆç½®ä¿¡åº¦ {probs['NS']}%ï¼‰" 
    #     output += f"{'T' if results['TF']==0 else 'F'}ï¼ˆç½®ä¿¡åº¦ {probs['TF']}%ï¼‰"
    #     output += f"{'J' if results['JP']==0 else 'P'}ï¼ˆç½®ä¿¡åº¦ {probs['JP']}%ï¼‰"
    #     return output

    def compute_word_importance(self, input_ids, model):
        embedding_layer = model.get_input_embeddings()
        embeddings = embedding_layer(input_ids)
        
        word_contributions = {}
        words = self.tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())
        
        for i, word in enumerate(words):
            if word in self.tokenizer.all_special_tokens:
                continue
                
            clean_word = word[1:] if word.startswith("â–") else word
            
            # åº”ç”¨æ–°çš„è¿‡æ»¤æ¡ä»¶
            if (
                len(clean_word) < 2 or
                "æˆ‘" in clean_word or
                (len(clean_word) <= 3 and clean_word.startswith("çš„")) or
                clean_word in self.stopwords 
                # (len(clean_word) < 3 and re.fullmatch(r'^[a-zA-Z]+$', clean_word))  # æ–°å¢è‹±æ–‡çŸ­è¯è¿‡æ»¤
            ):
                continue
                
            word_embedding = embeddings[0, i].cpu().detach().numpy()
            importance = np.linalg.norm(word_embedding)
            word_contributions[clean_word] = word_contributions.get(clean_word, 0) + importance
            
        return word_contributions

    def get_embedding_contributions(self, input_ids, attention_mask):
        total_contributions = {}
        for dim, model in self.models.items():
            contributions = self.compute_word_importance(input_ids, model)
            for word, score in contributions.items():
                total_contributions[word] = total_contributions.get(word, 0) + score
                
        top_30 = sorted(total_contributions.items(), key=lambda x: x[1], reverse=True)[:30]
        return top_30

# MBTITester ç±»å®šä¹‰
class MBTITester:
    def __init__(self):
        self.models = {}
        self.tokenizer = None
        self.stopwords = set([
            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 
            'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 
            'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 
            'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 
            'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 
            'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 
            'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 
            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 
            'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 
            'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 
            'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 
            'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 
            'wouldn','type','types','lot','people'
        ])
        self._load_models()

    def _load_single_model(self, dimension):
        model_path = os.path.join(MODEL_ROOT, f"{dimension}_model")
        
        if not self.tokenizer:
            self.tokenizer = BertTokenizer.from_pretrained(
                model_path,
                config=os.path.join(model_path, "config.json"),
                tokenizer_config_file=os.path.join(model_path, "tokenizer_config.json"),
                vocab_file=os.path.join(model_path, "vocab.txt")
            )
        
        model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True)
        return model.to(DEVICE)

    def _load_models(self):
        for dim in DIMENSIONS:
            self.models[dim] = self._load_single_model(dim)
            print(f"æˆåŠŸåŠ è½½ {dim} ç»´åº¦æ¨¡å‹")
        print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def predict(self, text):
        processed_text = preprocess_text(text)

        encoding = self.tokenizer(
            processed_text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        ).to(DEVICE)

        predictions = {}
        confidences = {}
        word_contributions = {}

        with torch.no_grad():
            for dim, model in self.models.items():
                outputs = model(**encoding)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)
                pred = torch.argmax(probs).item()
                confidence = int(probs[0][pred].item() * 100)

                predictions[dim] = pred
                confidences[dim] = confidence

                attention_weights = outputs.attentions
                word_contributions[dim] = self._compute_attention_contributions(attention_weights, encoding)

        total_word_contributions = self._aggregate_word_contributions(word_contributions)
        top_contributing_words = self._get_top_contributing_words(total_word_contributions)

        return self._format_mbti(predictions), confidences, top_contributing_words

    def _compute_attention_contributions(self, attention_weights, encoding):
        tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'][0].cpu().numpy())
        tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]', '[PAD]']]
        
        # æ·»åŠ ä¸predict.pyä¸€è‡´çš„è¿‡æ»¤é€»è¾‘
        meaningful_tokens = [
            token for token in tokens 
            if token.lower() not in self.stopwords 
            and not token.startswith('##') 
            and len(token) >= 3
        ]
        
        word_contributions = {token: 0 for token in meaningful_tokens}

        for layer in range(len(attention_weights)):
            for head in range(attention_weights[layer].shape[1]):
                attention_map = attention_weights[layer][0, head].detach().cpu().numpy()
                for i, token in enumerate(meaningful_tokens):
                    word_contributions[token] += np.sum(attention_map[i, :])
        
        return word_contributions

    def _aggregate_word_contributions(self, word_contributions):
        total_word_contributions = {}
        for dim, contributions in word_contributions.items():
            for word, contribution in contributions.items():
                if word not in total_word_contributions:
                    total_word_contributions[word] = 0
                total_word_contributions[word] += contribution
        return total_word_contributions

    # ä¿®æ”¹ MBTITester._get_top_contributing_words()
    def _get_top_contributing_words(self, total_word_contributions, top_n=30):  # ä¿®æ”¹top_nä¸º30
        return sorted(total_word_contributions.items(), key=lambda item: item[1], reverse=True)[:top_n]

    # æ·»åŠ ä¸predict.pyç›¸åŒçš„è¯äº‘ç”Ÿæˆæ–¹æ³•
    def generate_wordcloud(self, top_contributing_words):
        word_freq = {word: contribution for word, contribution in top_contributing_words}
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)
        plt.figure(figsize=(10, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        return plt

    def _format_mbti(self, predictions):
        type_map = {
            'IE': {0: 'I', 1: 'E'},
            'NS': {0: 'N', 1: 'S'},
            'TF': {0: 'T', 1: 'F'},
            'JP': {0: 'J', 1: 'P'}
        }
        return ''.join([type_map[dim][pred] for dim, pred in predictions.items()])

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
if 'tester' not in st.session_state:
    st.session_state.tester = MBTITester()
if 'bilingual_tester' not in st.session_state:
    st.session_state.bilingual_tester = BilingualMBTITester()

def get_image_base64(image_path):
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode()
    return encoded_string

st.markdown("""
    <style>
    [data-testid="stImage"] {
        margin-top: -60px !important;
    }
    </style>
    """, unsafe_allow_html=True)

st.image("images/beamer.svg",use_container_width=True)


# é¦–å…ˆå®šä¹‰ç»´åº¦å…¨ç§°çš„æ˜ å°„
DIMENSION_NAMES = {
    'I': 'Introverted',
    'E': 'Extraverted',
    'N': 'Intuition',
    'S': 'Sensing',
    'T': 'Thinking',
    'F': 'Feeling',
    'J': 'Judging',
    'P': 'Perceiving'
}

# å®šä¹‰æ¯ä¸ªç»´åº¦çš„é¢œè‰²
DIMENSION_COLORS = {
    'IE': ['#5996B1', '#5996B1'],  # çº¢è‰²åˆ°é’è‰²
    'NS': ['#DCB051', '#DCB051'],  # è–„è·ç»¿åˆ°æ©™è‰²
    'TF': ['#54A177', '#54A177'],  # ç»¿è‰²åˆ°ç²‰è‰²
    'JP': ['#826396', '#826396']   # çŠç‘šè‰²åˆ°è“è‰²
}

# æ›´æ–°æ ·å¼å®šä¹‰
st.markdown(
    """
    <style>
    .dimension-container {
        position: relative;
        margin: 30px 0;
        padding: 0 150px;  /* ä¸ºä¸¤ä¾§æ–‡æœ¬ç•™å‡ºç©ºé—´ */
    }
    
    .dimension-bar {
        width: 100%;
        height: 12px;
        border-radius: 6px;
        position: relative;
    }
    
    .dimension-indicator {
        width: 20px;
        height: 20px;
        background: white;
        border: 3px solid;
        border-radius: 50%;
        position: absolute;
        top: -4px;
        transform: translateX(-50%);
        cursor: pointer;
        transition: all 0.3s ease;
        z-index: 2;
    }
    
    .dimension-indicator:hover {
        transform: translateX(-50%) scale(1.2);
        box-shadow: 0 0 10px rgba(0,0,0,0.2);
    }
    
    .dimension-label {
    position: absolute;
    font-size: 14px;
    color: #2d3436;
    width: auto;  /* å›ºå®šå®½åº¦ */
    text-align: center;
    top: 50%;  
    transform: translateY(-50%);  /* è¿™é‡Œçš„å¼•å·æœ‰é—®é¢˜ï¼Œéœ€è¦ä¿®å¤ */
    }

    .dimension-label.left {
        left: 30px;  /* ä½¿ç”¨å›ºå®šè·ç¦»æ›¿ä»£transform */
        text-align: right;
    }

    .dimension-label.right {
        right: 30px;  /* ä½¿ç”¨å›ºå®šè·ç¦»æ›¿ä»£transform */
        text-align: left;
    }
    
    .dimension-value {
        position: absolute;
        font-size: 14px;
        font-weight: bold;
        transform: translateX(-50%);
        top: -40px;  /* ä¸Šç§»æ ‡ç­¾ */
        background: white;
        padding: 2px 8px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        min-width: 100px;  /* è®¾ç½®æœ€å°å®½åº¦ */
        width: auto;  /* å…è®¸è‡ªé€‚åº” */
        white-space: nowrap;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ä¿®æ”¹ç»´åº¦æ¡ç”Ÿæˆå‡½æ•°
def create_dimension_bar(left_type, right_type, confidence, pred, dim_key):
    # è·å–ç»´åº¦çš„é¢œè‰²
    colors = DIMENSION_COLORS[dim_key]
    # è·å–ç»´åº¦çš„å…¨ç§°
    left_full = DIMENSION_NAMES[left_type]
    right_full = DIMENSION_NAMES[right_type]
    # è®¡ç®—æŒ‡ç¤ºå™¨ä½ç½®
    position = confidence if pred == 1 else (100 - confidence)
    # è®¾ç½®æ¸å˜è‰²
    gradient = f"linear-gradient(to right, {colors[0]}, {colors[1]})"
    
    html = f"""
    <div class="dimension-container">
        <div class="dimension-label left">{left_full}</div>
        <div class="dimension-bar" style="background: {gradient};">
            <div class="dimension-value" style="left: {position}%; color: {colors[1] if pred == 1 else colors[0]}">
                {confidence}% {right_full if pred == 1 else left_full}
            </div>
            <div class="dimension-indicator" style="left: {position}%; border-color: {colors[1] if pred == 1 else colors[0]};"></div>
        </div>
        <div class="dimension-label right">{right_full}</div>
    </div>
    """
    return html


# æ·»åŠ å“åº”å¼æ–‡æœ¬æ ·å¼
st.markdown(
    """
    <style>
    button[kind="header"] {
        display: none;
    }
    * {
        font-family: "Source Sans Pro", "PingFang SC",  -apple-system, BlinkMacSystemFont, sans-serif !important;
    }
    .mbtitest-page-bigtitle {
        font-size: calc(36px + 0.25vw) !important;
        margin-bottom: calc(0px + 0vw) !important;
        font-weight: bold !important;
        margin-top: calc(-20px + 0.25vw) !important;
    }
    .mbtitest-page-title {
        font-size: calc(16px + 0.25vw) !important;
        margin-bottom: calc(8px + 0.25vw) !important;
        font-weight: bold !important;
    }
    .mbtitest-page-type {
        margin-top: calc(8px + 0.25vw) !important;
        font-size: calc(14px + 0.25vw) !important;
        margin-bottom: calc(8px + 0.25vw) !important;
        font-weight: bold !important;
    }
    .mbtitest-page-text {
        font-size: calc(12px + 0.25vw) !important;
        margin-bottom: calc(12px + 0.25vw) !important;
    }
    
    .mbtitest-card {
        background: white;
        border-radius: 20px;
        padding: 20px 20px 10px 20px;
        margin: 10px 10px 10px 10px;
        transition: all 0.3s ease;
        border: 1.5px solid #eee;
    }
    
    .mbtitest-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    .mbtitest-page-typetitle {
        font-size: calc(14px + 0.25vw) !important;
        font-weight: bold !important;
        color: #333;
        margin-bottom: 1px !important;
        text-align: center;
    }
    .mbtitest-page-typetitle2 {
        font-size: calc(13px + 0.25vw) !important;
        font-weight: bold !important;
        color: #333;
        margin-bottom: 15px !important;
        text-align: center;
    }
    .mbtitest-card-image {
        text-align: center;
        margin: 10px 0;
    }
    
    .mbtitest-card-text {
        font-size: calc(11px + 0.25vw) !important;
        color: #666;
        line-height: 1.6;
        margin: 10px 10px 10px 10px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown('<p class="mbtitest-page-bigtitle">ğŸ¯ MBTI æ€§æ ¼é¢„æµ‹</p>', unsafe_allow_html=True)
st.caption("ğŸš€ é€šè¿‡ä¸€æ®µæ–‡æœ¬ï¼ŒMBTInsightå¯ä»¥è‡ªåŠ¨é¢„æµ‹å‡ºä½ çš„MBTIæ€§æ ¼ç±»å‹ã€‚")

# æ–‡æœ¬è¾“å…¥åŒºåŸŸ
user_input = st.text_area(
    "åœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³è¦åˆ†æçš„æ–‡æœ¬:",
    height=150,
    placeholder="è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„ä¸­/è‹±æ–‡æ–‡æœ¬..."
)

# æ ¹æ®è¾“å…¥è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ¨¡å‹
default_model_index = 1 if contains_chinese(user_input) else 0

# æ¨¡å‹é€‰æ‹©
model_type = st.selectbox(
    "é€‰æ‹©é¢„æµ‹æ¨¡å‹:",
    ["è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§", "åŒè¯­ç‰ˆæ¨¡å‹ ğŸ‡¨ğŸ‡³ ğŸ‡¬ğŸ‡§"],
    index=default_model_index
)

# æ ¹æ®å½“å‰é€‰æ‹©çš„æ¨¡å‹ç±»å‹æ›´æ–°å ä½ç¬¦æ–‡æœ¬
placeholder_text = "è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„" + ("è‹±æ–‡æ–‡æœ¬..." if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§" else "ä¸­è‹±æ–‡æ–‡æœ¬...")

# ... å‰é¢çš„ä»£ç ä¿æŒä¸å˜ ...

# ä¿®æ”¹æ ·å¼å®šä¹‰éƒ¨åˆ†
st.markdown("""
    <style>
    /* ä¸º"å¼€å§‹åˆ†æ"æŒ‰é’®æ·»åŠ ç‰¹å®šæ ·å¼ */
    .start-analysis-button {
        background-color: #4CAF50 !important;
        color: white !important;
        font-weight: bold !important;
        padding: 10px 20px !important;
        border-radius: 5px !important;
        border: none !important;
        transition: all 0.3s ease !important;
        font-size: 16px !important;
        width: auto !important;
        text-align: center !important;
    }

    .start-analysis-button:hover {
        background-color: #45a049 !important;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1) !important;
    }
    </style>
""", unsafe_allow_html=True)

# å¼€å§‹æŒ‰é’®
if st.button("å¼€å§‹åˆ†æ",key="start-analysis",use_container_width=True, disabled=len(user_input.strip()) < 100 or (model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§" and contains_chinese(user_input))):
    if len(user_input.strip()) < 100:
        st.error("âš ï¸ è¯·è¾“å…¥è‡³å°‘100ä¸ªå­—ç¬¦çš„æ–‡æœ¬ä»¥ç¡®ä¿é¢„æµ‹å‡†ç¡®æ€§")
    elif model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§" and contains_chinese(user_input):
        st.error("âš ï¸ æ£€æµ‹åˆ°ä¸­æ–‡å­—ç¬¦ï¼è‹±æ–‡ç‰ˆæ¨¡å‹ä»…æ”¯æŒè‹±æ–‡è¾“å…¥ï¼Œè¯·åˆ‡æ¢åˆ°åŒè¯­ç‰ˆæ¨¡å‹æˆ–æ›´æ¢ä¸ºçº¯è‹±æ–‡æ–‡æœ¬ã€‚")
    else:
        with st.spinner("æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
            progress_bar = st.progress(0)
            
            try:
                # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç±»å‹è¿›è¡Œé¢„æµ‹
                if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§":
                    prediction, confidences, top_contributing_words = st.session_state.tester.predict(user_input)
                else:
                    prediction, confidences, top_contributing_words = st.session_state.bilingual_tester.predict(user_input)
                
                # æ›´æ–°è¿›åº¦æ¡
                for i in range(100):
                    time.sleep(0.02)
                    progress_bar.progress(i + 1)
                
                success = True
            except Exception as e:
                st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                success = False
            
            progress_bar.empty()
            
            if success:
                st.success("âœ¨ åˆ†æå®Œæˆï¼")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**é€‰ç”¨æ¨¡å‹ï¼š**")
                    st.markdown(model_type)
                with col2:
                    st.markdown("**é¢„æµ‹ç±»å‹ï¼š**")
                    st.markdown(f"**{prediction}**")

                st.session_state.mbti_type = prediction
                # æ˜¾ç¤ºå„ç»´åº¦ç½®ä¿¡åº¦
                st.markdown("**ç»´åº¦åˆ†æ**")

                dimension_pairs = [
                    ('I', 'E', 'IE'),
                    ('N', 'S', 'NS'),
                    ('T', 'F', 'TF'),
                    ('J', 'P', 'JP')
                ]
                
                # ä¿®æ”¹ç»´åº¦åˆ†ææ˜¾ç¤ºéƒ¨åˆ†
                for left, right, dim_key in dimension_pairs:
                    confidence = confidences[dim_key]
                    pred = 1 if prediction[dimension_pairs.index((left, right, dim_key))] == right else 0
                    html = create_dimension_bar(left, right, confidence, pred, dim_key)
                    st.markdown(html, unsafe_allow_html=True)

                
                if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§":
                    st.markdown("**å…³é”®è¯è¯äº‘åˆ†æ**")
                    st.markdown("å¯¹é¢„æµ‹ç»“æœå½±å“æœ€å¤§çš„è¯è¯­ï¼š")
                    
                    col1, col2, col3 = st.columns([1, 8, 1])
                    with col2:
                        st.markdown("<div style='margin-bottom:70px;'></div>", unsafe_allow_html=True)
                        with st.container():
                            plt = st.session_state.tester.generate_wordcloud(top_contributing_words)
                            st.pyplot(plt)
                            plt.close()
                else:
                    # æ›¿æ¢åŸæœ‰çš„å…³é”®è¯åˆ†æéƒ¨åˆ†
                    st.markdown("**å…³é”®è¯è¯äº‘åˆ†æ**")
                    st.markdown("å¯¹é¢„æµ‹ç»“æœå½±å“æœ€å¤§çš„è¯è¯­ï¼š")
                    
                    col1, col2, col3 = st.columns([1, 8, 1])

                    with col2:
                        st.markdown("<div style='margin-bottom:70px;'></div>", unsafe_allow_html=True)
                        with st.container():
                            # ç”Ÿæˆè¯äº‘
                            word_freq = {word: contri for word, contri in top_contributing_words}
                            wordcloud = WordCloud(
                                font_path="/System/Library/Fonts/Hiragino Sans GB.ttc",
                                width=500,  # é€‚å½“ç¼©å°ç”»å¸ƒå°ºå¯¸
                                height=200,
                                background_color='white',
                                scale=0.9
                            ).generate_from_frequencies(word_freq)
                        
                            plt.figure(figsize=(8, 4), dpi=300)  # è°ƒæ•´DPIé™ä½åˆ†è¾¨ç‡
                            plt.imshow(wordcloud, interpolation='bilinear')
                            plt.axis("off")
                            st.pyplot(plt, use_container_width=True)  # ä½¿ç”¨å®¹å™¨å®½åº¦è‡ªé€‚åº”
                            plt.close()

else:
    placeholder_text = "è‡³å°‘100ä¸ªå­—ç¬¦çš„" + ("è‹±æ–‡æ–‡æœ¬..." if model_type == "è‹±æ–‡ç‰ˆæ¨¡å‹ ğŸ‡¬ğŸ‡§ " else "ä¸­/è‹±æ–‡æ–‡æœ¬...")
    st.info(f"ğŸ‘† è¯·åœ¨ä¸Šæ–¹è¾“å…¥{placeholder_text}")

# åœ¨æ ·å¼å®šä¹‰éƒ¨åˆ†æ·»åŠ ä»¥ä¸‹CSS
st.markdown("""
    <style>
    /* ...existing code... */
    
    .keyword-container {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
    }
    
    .keyword-tag {
        background: #f1f3f5;
        padding: 3px 15px;
        border-radius: 15px;
        font-size: 14px;
        color: #495057;
        border: 1px solid #e9ecef;
        transition: all 0.2s ease;
    }
    
    .keyword-tag:hover {
        transform: translateY(-2px);
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    </style>
""", unsafe_allow_html=True)






# ====================================================
# åº•éƒ¨è·³è½¬
# ====================================================

st.divider()
col1, col2, col3 = st.columns(3)
# ä¿®æ”¹æ ·å¼ï¼Œæ·»åŠ æŒ‰é’®æ ·å¼

with col1:
    st.markdown('<p class="column-title">è¿”å›é¦–é¡µ</p>', unsafe_allow_html=True)
    
    with st.container():
        # ä½¿ç”¨ base64 ç¼–ç æ˜¾ç¤ºå›¾ç‰‡
        image_base64 = get_image_base64("images/col1.png")
        st.markdown(f"""
                    <a href="mbti" target="_self">
            <div class="card-container" onclick> 
                <img src="data:image/png;base64,{image_base64}" class="card-image">
            </div>
            </a>
        """, unsafe_allow_html=True)
        # ä½¿ç”¨å¯è§æŒ‰é’®
        if st.button('â† è¿”å›é¦–é¡µ', key='card1',type="tertiary"):
            st.switch_page("1_main.py")

with col2:
    st.markdown('<p class="column-title">äº†è§£ MBTI</p>', unsafe_allow_html=True)
    
    with st.container():
        # ä½¿ç”¨ base64 ç¼–ç æ˜¾ç¤ºå›¾ç‰‡
        image_base64 = get_image_base64("images/col2.png")
        st.markdown(f"""
                    <a href="mbti" target="_self">
            <div class="card-container" onclick> 
                <img src="data:image/png;base64,{image_base64}" class="card-image">
            </div>
            </a>
        """, unsafe_allow_html=True)
        if st.button('â† äº†è§£æ›´å¤š', key='card2' ,type="tertiary"):
            st.switch_page("2_mbti.py")

with col3:
    st.markdown('<p class="column-title">è¯¦ç»†è§£è¯»</p>', unsafe_allow_html=True)
    
    with st.container():
        # ä½¿ç”¨ base64 ç¼–ç æ˜¾ç¤ºå›¾ç‰‡
        image_base64 = get_image_base64("images/col3.png")
        st.markdown(f"""
                    <a href="result" target="_self">
            <div class="card-container" onclick> 
                <img src="data:image/png;base64,{image_base64}" class="card-image">
            </div>
            </a>
        """, unsafe_allow_html=True)
        # ä½¿ç”¨å¯è§æŒ‰é’®
        if st.button('äº†è§£æ›´å¤š â†’', key='card3',type="tertiary"):
            st.switch_page("5_result.py")

st.markdown("""
    <style>
    .card-container {
        cursor: pointer;
        transition: all 0.3s ease;
        # padding: 10px;
        border-radius: 10px;
        background: white;
        margin-bottom: 2px;
        width: 90%;
    }
    .card-container:hover {
        transform: translateY(-5px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    .card-image {
        width: 100%;
        border-radius: 8px;
        margin-bottom: 10px;
    }
    # /* è‡ªå®šä¹‰æŒ‰é’®æ ·å¼ */
    # .stButton>button {
    #     background-color: transparent !important;
    #     color: #1E88E5 !important;
    #     border: none !important;
    #     padding: 0 !important;
    #     font-weight: 500 !important;
    #     text-align: right !important;
    #     font-size: calc(10px + 0.2vw) !important;
    # }
    # .stButton>button p, .stButton>button span {
    #     font-size: calc(10px + 0.2vw) !important;  /* ç¡®ä¿æŒ‰é’®å†…éƒ¨æ–‡å­—ä¹Ÿä½¿ç”¨ç›¸åŒå¤§å° */
    # }
    # .stButton>button:hover {
    #     color: #1565C0 !important;
    #     background: none !important;
    #     border: none !important;
    # }
    .column-title {
        font-size: calc(14px + 0.3vw) !important;
        font-weight: bold !important;
        margin-bottom: calc(8px + 0.25vw) !important;
        margin-top: calc(-20px - 0.25vw) !important;
    }
    </style>
""", unsafe_allow_html=True)